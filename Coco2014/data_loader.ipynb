{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from build_vocab import Vocabulary\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    COCO Custom Dataset compatible with torch.utils.data.DataLoader.\n",
    "    自定义COCO dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns one data pair (image and caption).  \n",
    "        Return:\n",
    "             image: 经过transform处理 [3, 224, 224]\n",
    "             target: 经过序列化的caption\n",
    "        \"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]                            # 获取每一条annotation的id\n",
    "        caption = coco.anns[ann_id]['caption']       # 找到该annotation的caption\n",
    "        img_id = coco.anns[ann_id]['image_id']      # 找到该annotation的image_id\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']   # 根据img_id找到文件名\n",
    "        \n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())   # 提取每个单词并小写\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))                               # 添加开始和结束的标识\n",
    "        caption.extend([vocab(token) for token in tokens])      # 序列化caption\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    因为每个图片的caption长度不一致, 所以无法打包成mini-batch, 这里我们要手动增加pad补齐\n",
    "    在有多个标签的时候, 也需要自定义该方法\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).  padded_length是该batch中最长的caption长度\n",
    "        lengths: list; valid length for each padded caption. 每个caption的有效长度\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).  根据caption长度, 由大到小排列\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)                # 使image和caption分成两个list, \n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor). 增加一个维度, 为了制造mini-batch\n",
    "    images = torch.stack(images, 0)      #  [batch_size, 3, 224, 224]\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]                          # list, 记录每个caption的长度\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()    # 生成 [128, 18] 全零矩阵, 用于填充\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]            # 每个caption的结尾位置\n",
    "        targets[i, :end] = cap[:end]        # 覆盖每一行的前面部分     \n",
    "    return images, targets, lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '/Volumes/SD/Dataset/'\n",
    "vocab_path = dataset_root+'coco/vocab.pkl'   # path for vocabulary wrapper'\n",
    "image_dir = dataset_root+'coco/resized2014'  # directory for resized images'\n",
    "caption_path = dataset_root+'coco/annotations/captions_train2014.json'     # path for train annotation json file'\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "# Load vocabulary wrapper\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(crop_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.71s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = CocoDataset(root=image_dir,\n",
    "                   json=caption_path,\n",
    "                   vocab=vocab,\n",
    "                   transform=transform)\n",
    "\n",
    "# Data loader for COCO dataset\n",
    "# This will return (images, captions, lengths) for each iteration.\n",
    "# images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "# captions: a tensor of shape (batch_size, padded_length).\n",
    "# lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试 collate-fn方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.83s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(caption_path)\n",
    "data = []\n",
    "for index in range(10):\n",
    "    ann_id = list(coco.anns.keys())[index]                            # 获取每一条annotation的id\n",
    "    caption = coco.anns[ann_id]['caption']       # 找到该annotation的caption\n",
    "    img_id = coco.anns[ann_id]['image_id']      # 找到该annotation的image_id\n",
    "    path = coco.loadImgs(img_id)[0]['file_name']   # 根据img_id找到文件名\n",
    "\n",
    "    image = Image.open(os.path.join(image_dir, path)).convert('RGB')\n",
    "    image = transform(image)\n",
    "\n",
    "    # Convert caption (string) to word ids.\n",
    "    tokens = nltk.tokenize.word_tokenize(str(caption).lower())   # 提取每个单词并小写\n",
    "    caption = []\n",
    "    caption.append(vocab('<start>'))                               # 添加开始和结束的标识\n",
    "    caption.extend([vocab(token) for token in tokens])      # 序列化caption\n",
    "    caption.append(vocab('<end>'))\n",
    "    target = torch.Tensor(caption)\n",
    "    \n",
    "    data.append([image, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Sort a data list by caption length (descending order).  根据caption长度, 由大到小排列\n",
    "data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "images, captions = zip(*data)                # 使image和caption分成两个list, \n",
    "\n",
    "# Merge images (from tuple of 3D tensor to 4D tensor). 增加一个维度, 为了制造mini-batch\n",
    "images = torch.stack(images, 0)      #  [batch_size, 3, 224, 224]\n",
    "\n",
    "# Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "lengths = [len(cap) for cap in captions]                          # list, 记录每个caption的长度\n",
    "targets = torch.zeros(len(captions), max(lengths)).long()    # 生成 [128, 18] 全零矩阵, 用于填充\n",
    "for i, cap in enumerate(captions):\n",
    "    end = lengths[i]            # 每个caption的结尾位置\n",
    "    targets[i, :end] = cap[:end]        # 覆盖每一行的前面部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
